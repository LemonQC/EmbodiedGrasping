<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <link rel="alternate icon" type="image/png" href="/EmbodiedGrasping/img/favicon-32x32.png">
    <title>EmbodiedGrasping | </title>
    
<link rel="stylesheet" href="/EmbodiedGrasping/css/reset.css">

    
<link rel="stylesheet" href="/EmbodiedGrasping/css/style.css">

    
<link rel="stylesheet" href="/EmbodiedGrasping/css/markdown.css">

    
<link rel="stylesheet" href="/EmbodiedGrasping/css/fonts.css">


    <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
<meta name="generator" content="Hexo 6.3.0"></head>
    <body>
        <div class="paper">
            <div class="paper-main">

                <div class="post-main">

    
    
        <div class="post-main-title">
            Think Before Execute: Embodied Reasoning of Supportive Tools for Zero-shot Tasks with Large Language Models
        </div>
        <div class="post-meta">
            2024-02-28
        </div>
        <div class="post-md">
            <h2 id="Overview-of-framework"><a href="#Overview-of-framework" class="headerlink" title="Overview of framework"></a>Overview of framework</h2><p>Architecture of the overall framework. LLMs play important role in three modules, namely language comprehension module, embodied constraint judgement module, and tool utilization module. Conditioned on the retrieved scene graph knowledge and its own skills (including manipulation skills and movement skills), the robot first conduct the semantic analysis of human intention and task feasibility using LLMs. Once the robot reaches the target position, it assesses its capabilities and physical parameters to ascertain if it can accomplish the task autonomously without the need for tools. When tools are required, the robot utilizes a scene knowledge graph to search for appropriate tools and accomplish the given task. After the task is completed, the robot will proceed to update the current scene graph to facilitate the execution of subsequent tasks..<br><img src="/EmbodiedGrasping/./images/overall.jpg" alt="listen-perceive-grasp paradigm"></p>
<h2 id="Other-qualitative-results-in-real-world-scene"><a href="#Other-qualitative-results-in-real-world-scene" class="headerlink" title="Other qualitative results in real-world scene"></a>Other qualitative results in real-world scene</h2><h3 id="Pouring-soy-sauce"><a href="#Pouring-soy-sauce" class="headerlink" title="Pouring soy sauce"></a>Pouring soy sauce</h3><html><div class="video-container"><iframe width="560" height="315" src="https://www.youtube.com/embed/riViZrUvzyo?si=oXIfTdbQqQwZhQbU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></div></html>

<h3 id="Grasping-one-apple"><a href="#Grasping-one-apple" class="headerlink" title="Grasping one apple"></a>Grasping one apple</h3><html><div class="video-container"><iframe width="560" height="315" src="https://www.youtube.com/embed/2tUjCtWA0Io?si=-j9FixJnygn9g-vf" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></div></html>


        </div>

    

</div>
                <div class="footer">
    <span>Copyright Â© 2023 EmbodiedGrasping</span>
    <span>Designed By <a target="_blank" href="https://github.com/xiejialong/CTNet">Jialong Xie</a></span>
</div>


<link rel="stylesheet" href="/EmbodiedGrasping/css/a11y-dark.min.css">


<script src="/EmbodiedGrasping/js/highlight.min.js"></script>


<script src="/EmbodiedGrasping/js/highlightjs-line-numbers.js"></script>


<script>
    hljs.initHighlightingOnLoad();
    hljs.initLineNumbersOnLoad();
</script>

            </div>
        </div>
    </body>
</html>