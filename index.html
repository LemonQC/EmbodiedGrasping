<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <link rel="alternate icon" type="image/png" href="/EmbodiedGrasping/img/favicon-32x32.png">
    <title>CTNet | </title>
    
<link rel="stylesheet" href="/EmbodiedGrasping/css/reset.css">

    
<link rel="stylesheet" href="/EmbodiedGrasping/css/style.css">

    
<link rel="stylesheet" href="/EmbodiedGrasping/css/markdown.css">

    
<link rel="stylesheet" href="/EmbodiedGrasping/css/fonts.css">


    <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
<meta name="generator" content="Hexo 6.3.0"></head>
    <body>
        <div class="paper">
            <div class="paper-main">

                <div class="post-list">
    
    <div class="post">
        <!-- <a class="post-title" href="/EmbodiedGrasping/2024/02/28/paper/">Think Before Execute: Embodied Reasoning of Supportive Tools for Zero-shot Tasks with Large Language Models</a> -->
        <div class="post-title" >Think Before Execute: Embodied Reasoning of Supportive Tools for Zero-shot Tasks with Large Language Models</div>
        <div class="post-authors" >Jin Liu, Jialong Xie, Leibing Xiao, Chaoqun Wang, Fengyu Zhou</div>
        <div class="post-affiliations" >Shandong University</div>
        <div class="head-icon"> 

            

            
                <a class="icon-container" target="_blank" rel="noopener" href="https://youtu.be/aVLP5_yC8l0">
                    <i class="fa fa-youtube-square"></i>
                </a>
            
            
        </div>
        <div class="line"></div>
        <div class="post-abstract-title">Abstract</div>
        <div class="post-abstract" >Large language models (LLMs) benefit from their rich commonsense knowledge and have been widely used for diverse zero-shot robotic tasks where no extra detailed instructions are provided. Nevertheless, they often fail to adequately assess the feasibility of task execution due to insufficient consideration of the robot's capabilities and scene knowledge, potentially leading to tool misuse or task failure. In this paper, we leverage the language processing capabilities of large models for semantic parsing, making them the central hub for zero-shot task decision-making and execution. Subsequently, during the task execution, we develop an embodied judgment module for robots based on LLMs to conduct its task executable analysis in the current scene, ensuring that the robot can rely on its own abilities or utilize reasonable tools to address tasks. Additionally, the robot updates the scene knowledge graph to enhance the accuracy and efficiency of subsequent executions after completing each task. Extensive experiments conducted in various real-world scenes demonstrate the effectiveness and generalization of our proposed framework.</div>
        
        
            <div class="video-container">
                <iframe
                width="560"
                height="315"
                src="https://www.youtube.com/embed/aVLP5_yC8l0"
                frameborder="0"
                allowfullscreen
                ></iframe>
            </div>
        
        <div class="post-except">
            <h2 id="Overview-of-CTNet"><a href="#Overview-of-CTNet" class="headerlink" title="Overview of CTNet"></a>Overview of CTNet</h2><p>Architecture of the overall framework. LLMs play important role in three modules, namely language comprehension module, embodied constraint judgement module, and tool utilization module. Conditioned on the retrieved scene graph knowledge and its own skills (including manipulation skills and movement skills), the robot first conduct the semantic analysis of human intention and task feasibility using LLMs. Once the robot reaches the target position, it assesses its capabilities and physical parameters to ascertain if it can accomplish the task autonomously without the need for tools. When tools are required, the robot utilizes a scene knowledge graph to search for appropriate tools and accomplish the given task. After the task is completed, the robot will proceed to update the current scene graph to facilitate the execution of subsequent tasks..<br><img src="/EmbodiedGrasping/./images/overall.jpg" alt="listen-perceive-grasp paradigm"></p>
<h2 id="Qualitative-results-in-real-world-scene"><a href="#Qualitative-results-in-real-world-scene" class="headerlink" title="Qualitative results in real-world scene"></a>Qualitative results in real-world scene</h2><iframe src="https://youtu.be/GlX4P10OPbM" frameborder="0" allowfullscreen="" width='640' height='480'></iframe>


            <!-- <a class="read-more" href="/EmbodiedGrasping/2024/02/28/paper/"> ... </a> -->
        </div>
        <div class="post-date">2024.02.28</div>
    </div>
    
</div>

                <div class="footer">
    <span>Copyright Â© 2023 CTNet</span>
    <span>Designed By <a target="_blank" href="https://github.com/xiejialong/CTNet">Jialong Xie</a></span>
</div>


<link rel="stylesheet" href="/EmbodiedGrasping/css/a11y-dark.min.css">


<script src="/EmbodiedGrasping/js/highlight.min.js"></script>


<script src="/EmbodiedGrasping/js/highlightjs-line-numbers.js"></script>


<script>
    hljs.initHighlightingOnLoad();
    hljs.initLineNumbersOnLoad();
</script>

            </div>
        </div>
    </body>
</html>