<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <link rel="alternate icon" type="image/png" href="/EmbodiedGrasping/img/favicon-32x32.png">
    <title>CTNet | </title>
    
<link rel="stylesheet" href="/EmbodiedGrasping/css/reset.css">

    
<link rel="stylesheet" href="/EmbodiedGrasping/css/style.css">

    
<link rel="stylesheet" href="/EmbodiedGrasping/css/markdown.css">

    
<link rel="stylesheet" href="/EmbodiedGrasping/css/fonts.css">


    <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
<meta name="generator" content="Hexo 6.3.0"></head>
    <body>
        <div class="paper">
            <div class="paper-main">

                <div class="post-list">
    
    <div class="post">
        <!-- <a class="post-title" href="/EmbodiedGrasping/2024/02/28/paper/">Think more before Execute: Large Language Models Empower Robots with Embodied Tool Usage for Zero-shot Tasks</a> -->
        <div class="post-title" >Think more before Execute: Large Language Models Empower Robots with Embodied Tool Usage for Zero-shot Tasks</div>
        <div class="post-authors" >Jin Liu, Jialong Xie, Leibing Xiao, Chaoqun Wang, Fengyu Zhou</div>
        <div class="post-affiliations" >Shandong University</div>
        <div class="head-icon"> 

            

            
                <a class="icon-container" target="_blank" rel="noopener" href="https://youtu.be/aVLP5_yC8l0">
                    <i class="fa fa-youtube-square"></i>
                </a>
            
            
        </div>
        <div class="line"></div>
        <div class="post-abstract-title">Abstract</div>
        <div class="post-abstract" >Large language models (LLMs) benefit from their rich commonsense knowledge and have been widely used for diverse zero-shot robotic tasks where no extra detailed instructions are provided. Nevertheless, they often fail to adequately assess the feasibility of task execution due to insufficient consideration of the robot's capabilities and scene knowledge, potentially leading to tool misuse or task failure. In this paper, we leverage the language processing capabilities of large models for semantic parsing, making them the central hub for zero-shot task decision-making and execution. Subsequently, during the task execution, we develop an embodied judgment module for robots based on LLMs to conduct its task executable analysis in the current scene, ensuring that the robot can rely on its own abilities or utilize reasonable tools to address tasks. Additionally, the robot updates the scene knowledge graph to enhance the accuracy and efficiency of subsequent executions after completing each task. Extensive experiments conducted in various real-world scenes demonstrate the effectiveness and generalization of our proposed framework.</div>
        
        
            <div class="video-container">
                <iframe
                width="560"
                height="315"
                src="https://www.youtube.com/embed/aVLP5_yC8l0"
                frameborder="0"
                allowfullscreen
                ></iframe>
            </div>
        
        <div class="post-except">
            <h2 id="Overview-of-CTNet"><a href="#Overview-of-CTNet" class="headerlink" title="Overview of CTNet"></a>Overview of CTNet</h2><p>An overview of CTNet architecture. Given an image-query pair, we first employ listen module to understand and align visual and linguistic concepts. Then, the perceive module recursively extracts object-orientied features with visual attributes and generates the target mask corresponding to the description. Last,  The grasp module aggregates the attribute-based and semantic-rich features to refine the grasp pose of the desired object.<br><img src="/EmbodiedGrasping/./images/architecture.jpg" alt="listen-perceive-grasp paradigm"><br><img src="/EmbodiedGrasping/./images/TAMMI.jpg" alt="TAMMI"></p>
<iframe src="//www.youtube.com/embed/IfjVsa1t2Jw" frameborder="0" allowfullscreen="" width='640' height='480'></iframe> -->
            <!-- <a class="read-more" href="/EmbodiedGrasping/2024/02/28/paper/"> ... </a> -->
        </div>
        <div class="post-date">2024.02.28</div>
    </div>
    
</div>

                <div class="footer">
    <span>Copyright Â© 2023 CTNet</span>
    <span>Designed By <a target="_blank" href="https://github.com/xiejialong/CTNet">Jialong Xie</a></span>
</div>


<link rel="stylesheet" href="/EmbodiedGrasping/css/a11y-dark.min.css">


<script src="/EmbodiedGrasping/js/highlight.min.js"></script>


<script src="/EmbodiedGrasping/js/highlightjs-line-numbers.js"></script>


<script>
    hljs.initHighlightingOnLoad();
    hljs.initLineNumbersOnLoad();
</script>

            </div>
        </div>
    </body>
</html>